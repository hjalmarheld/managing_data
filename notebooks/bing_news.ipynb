{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import bs4 as bs\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# FETCHING SETUP\n",
    "#\n",
    "# api params and url - key needed from azure\n",
    "# https://www.microsoft.com/en-us/bing/apis/bing-web-search-api\n",
    "\n",
    "subscription_key = \"<key goes here>\"\n",
    "\n",
    "search_url = \"https://api.bing.microsoft.com/v7.0/news/search\"\n",
    "headers = {\"Ocp-Apim-Subscription-Key\" : subscription_key}\n",
    "params  = {\"q\": 'placeholder', \"textDecorations\": True, \"textFormat\": \"HTML\", 'count':100, 'mkt': 'fr-FR'}\n",
    "\n",
    "def search(search_term):\n",
    "    params[\"q\"] = search_term\n",
    "    response = requests.get(search_url, headers=headers, params=params)\n",
    "    response.raise_for_status()\n",
    "    df = pd.json_normalize(response.json())\n",
    "    df = pd.DataFrame(df['value'][0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# DATA FETCHING\n",
    "#\n",
    "# loop over all companies,dump in case of loop failure\n",
    "\n",
    "companies = pd.read_csv('data/external/top_companies.txt', sep='\t')\n",
    "all_data = []\n",
    "for company in tqdm(companies['Entreprise']):\n",
    "    df = search(company)\n",
    "    df['company'] = company\n",
    "    all_data.append(df)\n",
    "    df.to_csv('dump/%s.csv' % company)\n",
    "\n",
    "# merge all articles and company data\n",
    "all_articles = pd.concat(all_data)\n",
    "all_articles = all_articles.merge(\n",
    "    companies[['Entreprise','Activité']],\n",
    "    left_on='company', right_on='Entreprise'\n",
    "    ).drop('Entreprise', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# DATA CLEANUP AND SAVE\n",
    "#\n",
    "\n",
    "# remove html characters\n",
    "def clean_html(x):\n",
    "    soup = bs.BeautifulSoup(x, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "all_articles['name_clean'] = all_articles['name'].apply(clean_html)\n",
    "all_articles['clean_description'] = all_articles['description'].apply(clean_html)\n",
    "\n",
    "# add naf to articles\n",
    "all_articles['naf_clean'] =  all_articles['Activité'].apply(lambda x: x.split('(')[-1].split(')')[0])\n",
    "mapping = pd.read_csv('https://www.data.gouv.fr/fr/datasets/r/7bb2184b-88cb-4c6c-a408-5a0081816dcd', sep=',')[['id_2', 'id_5']].set_index('id_5')\n",
    "all_articles = all_articles.merge(mapping, left_on='naf_clean', right_index=True)\n",
    "\n",
    "# remove excessive data\n",
    "clean_articles = all_articles[['company', 'name_clean', 'clean_description','id_2']]\n",
    "clean_articles.columns = ['company','title','text','naf']\n",
    "clean_articles.to_csv('data/external/labelled articles cleaned.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ds2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31d8a1ef34cbcebda31b01d96f0668cc5da4b9d1ec201e7411d63c9c214da37f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
